#!/usr/bin/env python3
"""
02c_quantify_geometry_rois.py - Full PET Quantification from Geometry ROIs

Extracts SUV, TPR, and FUR metrics from per-tooth geometry ROIs
generated by 02b_geometry_roi_poc.py.

This script:
1. Loads geometry ROI (in cropped CT space)
2. Resamples ROIs to PET native space (and saves for QC overlay)
3. Extracts raw intensity (Bq/mL) per tooth
4. Computes SUV using weight + dose from eCRF
5. Computes TPR using time-averaged plasma concentration
6. Computes FUR using cumulative plasma AUC
7. Saves tooth-level + jaw-aggregate CSVs to Outputs/

Cross-session harmonization:
    For paired Baseline/Followup analysis, teeth excluded in EITHER
    session are excluded from BOTH (greedy exclusion). This ensures
    the weighted average is computed over the same set of teeth.

Usage:
    python 02c_quantify_geometry_rois.py --subject sub-103 --session ses-oqbgk
"""

import argparse
import json
import sys
import logging
from pathlib import Path
from datetime import datetime

import numpy as np
import nibabel as nib
import pandas as pd
from scipy.ndimage import map_coordinates
from numpy.linalg import inv
from scipy.interpolate import interp1d

# Add Scripts directory to path
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

from config import (
    RAWDATA_DIR, ROI_DIR, TOTALSEG_ROI_DIR, ROI_QC_DIR, OUTPUTS_DIR, LOGNOTES_DIR,
    TOTALSEG_SEG_DIR, BLINDKEY_DIR, DERIVED_DIR, TRANSFORM_DIR,
    ensure_directories
)
from utils.io_utils import (
    load_nifti, save_nifti, get_voxel_dimensions, get_voxel_volume_ml,
    load_blinding_key
)

logger = logging.getLogger(__name__)


# =============================================================================
# BLINDING KEY HELPERS
# =============================================================================

def get_timepoint(subject_id: str, session_id: str, blinding_map: dict) -> str:
    """Look up timepoint (Baseline/Followup) from blinding key."""
    key = (subject_id, session_id)
    tp = blinding_map.get(key, None)
    if tp is None:
        raise ValueError(f"No blinding key entry for {subject_id}/{session_id}")
    return tp


def get_partner_session(subject_id: str, session_id: str, blinding_map: dict) -> str:
    """Find the other session (Baseline↔Followup) for this subject."""
    my_tp = get_timepoint(subject_id, session_id, blinding_map)
    target_tp = "Followup" if my_tp == "Baseline" else "Baseline"
    for (sid, sess), tp in blinding_map.items():
        if sid == subject_id and tp == target_tp:
            return sess
    return None


# =============================================================================
# eCRF DATA
# =============================================================================

def load_ecrf_data() -> pd.DataFrame:
    """Load eCRF CSV (weight, dose, etc.)."""
    ecrf_dir = RAWDATA_DIR / "eCRF_data"
    ecrf_files = sorted(ecrf_dir.glob("K8ERAPKIH22001_DATA_*.csv"))
    if not ecrf_files:
        raise FileNotFoundError(f"No eCRF CSV found in {ecrf_dir}")
    ecrf_file = ecrf_files[-1]  # most recent
    logger.info(f"Loading eCRF: {ecrf_file.name}")
    return pd.read_csv(ecrf_file)


def get_suv_parameters(ecrf_df: pd.DataFrame, subject_id: str, timepoint: str) -> dict:
    """Extract weight_kg and dose_MBq from eCRF for SUV calculation."""
    subj_num = int(subject_id.replace('sub-', ''))
    row = ecrf_df[ecrf_df['subject_id'] == subj_num]
    if len(row) == 0:
        # Try record_id
        row = ecrf_df[ecrf_df['record_id'] == subj_num]
    if len(row) == 0:
        raise ValueError(f"Subject {subject_id} not found in eCRF")
    row = row.iloc[0]

    if timepoint == "Baseline":
        weight_col = 'weight_kg_pet_1'
        dose_col = 'injected_mbq_pet_1'
    else:
        weight_col = 'weight_kg_pet_2'
        dose_col = 'injected_mbq_pet_2'

    weight = row.get(weight_col, np.nan)
    dose = row.get(dose_col, np.nan)

    # Handle European comma format
    for val_name, val in [('weight', weight), ('dose', dose)]:
        if isinstance(val, str):
            val = float(val.replace(',', '.'))
            if val_name == 'weight':
                weight = val
            else:
                dose = val

    weight = float(weight) if not pd.isna(weight) else np.nan
    dose = float(dose) if not pd.isna(dose) else np.nan

    logger.info(f"SUV params: weight={weight}kg, dose={dose}MBq ({timepoint})")
    return {'weight_kg': weight, 'dose_mbq': dose}


# =============================================================================
# PET JSON METADATA
# =============================================================================

def load_pet_json(subject_id: str, timepoint: str) -> dict:
    """Load PET JSON sidecar for scan timing."""
    json_dir = RAWDATA_DIR / "json_side_cars_updated"
    pattern = f"{subject_id}_ses-{timepoint}_trc-18FFDG_rec-StaticMoCo_chunk-1_pet.json"
    json_file = json_dir / pattern
    if not json_file.exists():
        # Try without chunk
        pattern2 = f"{subject_id}_ses-{timepoint}_trc-18FFDG_rec-StaticMoCo_pet.json"
        json_file = json_dir / pattern2
    if not json_file.exists():
        logger.warning(f"PET JSON not found: {json_file}")
        return {}
    with open(json_file) as f:
        return json.load(f)


# =============================================================================
# INPUT FUNCTION
# =============================================================================

def load_input_function(subject_id: str, timepoint: str) -> dict:
    """
    Load processed input function and compute TPR/FUR denominators.

    TPR denominator: time-averaged plasma concentration over scan window
        C_plasma_mean = (1/(t2-t1)) * integral(t1 to t2) C_plasma(t) dt

    FUR denominator: cumulative plasma AUC from 0 to tissue measurement time
        AUC_0_to_T = integral(0 to T) C_plasma(t) dt
    """
    if_dir = DERIVED_DIR / "input_functions"
    if_file = if_dir / f"{subject_id}_ses-{timepoint}_if_processed.csv"

    if not if_file.exists():
        logger.warning(f"Input function not found: {if_file}")
        return None

    df = pd.read_csv(if_file)
    times = df['time_s'].values
    activities = df['activity_Bq_mL'].values

    # Interpolate
    f_interp = interp1d(times, activities, kind='linear', fill_value='extrapolate',
                        bounds_error=False)

    return {
        'times': times,
        'activities': activities,
        'interp_func': f_interp,
    }


def compute_plasma_denominators(if_data: dict, scan_start_s: float, scan_end_s: float,
                                 tissue_time_s: float = None) -> dict:
    """
    Compute TPR and FUR denominators from input function.

    Args:
        if_data: dict from load_input_function
        scan_start_s: scan start time (seconds post-injection)
        scan_end_s: scan end time
        tissue_time_s: tissue measurement time for FUR (mid-scan if None)

    Returns:
        dict with plasma_mean_Bq_mL, plasma_auc_window, plasma_auc_0_to_T
    """
    if tissue_time_s is None:
        tissue_time_s = (scan_start_s + scan_end_s) / 2

    f = if_data['interp_func']

    # TPR denominator: time-averaged plasma in scan window
    window_t = np.arange(scan_start_s, scan_end_s, 1.0)
    window_c = f(window_t)
    window_c[window_c < 0] = 0  # no negative concentrations
    auc_window = float(np.trapz(window_c, window_t))
    plasma_mean = auc_window / max(1, scan_end_s - scan_start_s)

    # FUR denominator: cumulative AUC from 0 to T
    full_t = np.arange(0, tissue_time_s, 1.0)
    full_c = f(full_t)
    full_c[full_c < 0] = 0
    auc_0_to_T = float(np.trapz(full_c, full_t))

    # Count actual data samples in scan window
    times = if_data['times']
    n_in_window = int(np.sum((times >= scan_start_s) & (times <= scan_end_s)))

    logger.info(f"Plasma denominators: mean={plasma_mean:.1f} Bq/mL, "
                f"AUC_window={auc_window:.0f}, AUC_0_to_T={auc_0_to_T:.0f}, "
                f"samples_in_window={n_in_window}")

    return {
        'plasma_mean_Bq_mL': plasma_mean,
        'plasma_auc_window_Bq_s_mL': auc_window,
        'plasma_auc_0_to_T_Bq_s_mL': auc_0_to_T,
        'tissue_time_s': tissue_time_s,
        'n_samples_in_window': n_in_window,
    }


# =============================================================================
# SPATIAL RESAMPLING
# =============================================================================

def resample_pet_to_roi(pet_data: np.ndarray, pet_img: nib.Nifti1Image,
                          roi_img: nib.Nifti1Image) -> np.ndarray:
    """
    Resample PET from native space to ROI (cropped-CT) space.
    Uses trilinear interpolation. Legacy fallback only.
    """
    roi_shape = roi_img.shape[:3]
    roi_affine = roi_img.affine
    pet_affine = pet_img.affine

    ii, jj, kk = np.mgrid[0:roi_shape[0], 0:roi_shape[1], 0:roi_shape[2]]
    roi_vox = np.stack([ii.ravel(), jj.ravel(), kk.ravel(), np.ones(ii.size)], axis=0)

    roi2world = roi_affine
    world2pet = inv(pet_affine)
    roi2pet = world2pet @ roi2world

    pet_vox = roi2pet @ roi_vox

    pet_resampled = map_coordinates(
        pet_data, pet_vox[:3], order=1, mode='constant', cval=0
    ).reshape(roi_shape)

    return pet_resampled


# =============================================================================
# METRIC EXTRACTION
# =============================================================================

def extract_tooth_metrics_weighted(pet_data: np.ndarray, continuous_masks: dict,
                                    suv_params: dict, plasma_denom: dict,
                                    voxel_vol_ml: float,
                                    weight_threshold: float = 0.05) -> list:
    """
    Extract intensity, SUV, TPR, FUR per tooth using continuous (partial-volume)
    masks in PET native space.

    Each per-tooth mask has float values in [0,1] from linear interpolation.
    Voxels with weight > threshold contribute to the weighted mean/percentiles.

    Args:
        pet_data: PET data array (native space)
        continuous_masks: dict {fdi_int: weight_array} continuous masks per tooth
        suv_params: dict with weight_kg, dose_mbq
        plasma_denom: dict with plasma denominators (or None)
        voxel_vol_ml: voxel volume in mL
        weight_threshold: minimum weight to include a voxel (default 0.05)

    Returns list of dicts (one per tooth + jaw aggregates).
    """
    weight_kg = suv_params.get('weight_kg', np.nan)
    dose_mbq = suv_params.get('dose_mbq', np.nan)

    if np.isfinite(weight_kg) and np.isfinite(dose_mbq) and dose_mbq > 0:
        suv_scaler = weight_kg / (dose_mbq * 1000)
    else:
        suv_scaler = np.nan

    plasma_mean = plasma_denom.get('plasma_mean_Bq_mL', np.nan) if plasma_denom else np.nan
    auc_0_to_T = plasma_denom.get('plasma_auc_0_to_T_Bq_s_mL', np.nan) if plasma_denom else np.nan

    records = []

    # Per-tooth weighted metrics
    for fdi in sorted(continuous_masks.keys()):
        weights = continuous_masks[fdi]
        mask = weights > weight_threshold
        if not np.any(mask):
            continue

        pet_vals = pet_data[mask]
        w = weights[mask]

        # Exclude non-finite and zero PET
        valid_idx = np.isfinite(pet_vals) & (pet_vals > 0)
        if not np.any(valid_idx):
            continue

        pet_valid = pet_vals[valid_idx]
        w_valid = w[valid_idx]

        # Weighted mean
        intensity_wmean = float(np.average(pet_valid, weights=w_valid))
        # Unweighted for comparison / percentiles
        intensity_median = float(np.median(pet_valid))
        intensity_p90 = float(np.percentile(pet_valid, 90))
        intensity_max = float(np.max(pet_valid))
        intensity_std = float(np.std(pet_valid))

        # Effective volume: sum of weights * voxel_vol
        effective_vol = float(np.sum(w_valid)) * voxel_vol_ml

        # SUV
        suv_wmean = intensity_wmean * suv_scaler if np.isfinite(suv_scaler) else np.nan
        suv_median = intensity_median * suv_scaler if np.isfinite(suv_scaler) else np.nan
        suv_p90 = intensity_p90 * suv_scaler if np.isfinite(suv_scaler) else np.nan
        suv_max = intensity_max * suv_scaler if np.isfinite(suv_scaler) else np.nan

        # TPR
        if np.isfinite(plasma_mean) and plasma_mean > 0:
            tpr_wmean = intensity_wmean / plasma_mean
            tpr_p90 = intensity_p90 / plasma_mean
        else:
            tpr_wmean = np.nan
            tpr_p90 = np.nan

        # FUR
        if np.isfinite(auc_0_to_T) and auc_0_to_T > 0:
            fur_wmean = (intensity_wmean / auc_0_to_T) * 60
            fur_p90 = (intensity_p90 / auc_0_to_T) * 60
        else:
            fur_wmean = np.nan
            fur_p90 = np.nan

        jaw = 'upper' if 11 <= fdi <= 28 else 'lower'

        records.append({
            'fdi_tooth': int(fdi),
            'jaw': jaw,
            'n_voxels': int(np.sum(valid_idx)),
            'n_voxels_excluded_zero': int(np.sum(~valid_idx)),
            'roi_volume_ml': effective_vol,
            'intensity_mean_Bq_mL': intensity_wmean,
            'intensity_median_Bq_mL': intensity_median,
            'intensity_p90_Bq_mL': intensity_p90,
            'intensity_max_Bq_mL': intensity_max,
            'intensity_std_Bq_mL': intensity_std,
            'SUV_mean': suv_wmean,
            'SUV_median': suv_median,
            'SUV_p90': suv_p90,
            'SUV_max': suv_max,
            'TPR_mean': tpr_wmean,
            'TPR_p90': tpr_p90,
            'FUR_mean_per_min': fur_wmean,
            'FUR_p90_per_min': fur_p90,
        })

    # Jaw-level aggregates (pool all voxels from teeth in that jaw)
    for jaw_name, fdi_range in [('upper_jaw', range(11, 29)), ('lower_jaw', range(31, 49))]:
        # Combine all per-tooth masks in this jaw
        jaw_weight = None
        for fdi in sorted(continuous_masks.keys()):
            if fdi not in fdi_range:
                continue
            w = continuous_masks[fdi]
            if jaw_weight is None:
                jaw_weight = w.copy()
            else:
                jaw_weight = np.maximum(jaw_weight, w)

        if jaw_weight is None:
            continue

        mask = jaw_weight > weight_threshold
        if not np.any(mask):
            continue

        pet_vals = pet_data[mask]
        valid = pet_vals[np.isfinite(pet_vals) & (pet_vals > 0)]
        if len(valid) == 0:
            continue

        intensity_mean = float(np.mean(valid))
        intensity_p90 = float(np.percentile(valid, 90))

        records.append({
            'fdi_tooth': jaw_name,
            'jaw': jaw_name.replace('_jaw', ''),
            'n_voxels': len(valid),
            'n_voxels_excluded_zero': int(np.sum(pet_vals == 0)),
            'roi_volume_ml': float(np.sum(jaw_weight[mask])) * voxel_vol_ml,
            'intensity_mean_Bq_mL': intensity_mean,
            'intensity_median_Bq_mL': float(np.median(valid)),
            'intensity_p90_Bq_mL': intensity_p90,
            'intensity_max_Bq_mL': float(np.max(valid)),
            'intensity_std_Bq_mL': float(np.std(valid)),
            'SUV_mean': intensity_mean * suv_scaler if np.isfinite(suv_scaler) else np.nan,
            'SUV_median': float(np.median(valid)) * suv_scaler if np.isfinite(suv_scaler) else np.nan,
            'SUV_p90': intensity_p90 * suv_scaler if np.isfinite(suv_scaler) else np.nan,
            'SUV_max': float(np.max(valid)) * suv_scaler if np.isfinite(suv_scaler) else np.nan,
            'TPR_mean': intensity_mean / plasma_mean if (np.isfinite(plasma_mean) and plasma_mean > 0) else np.nan,
            'TPR_p90': intensity_p90 / plasma_mean if (np.isfinite(plasma_mean) and plasma_mean > 0) else np.nan,
            'FUR_mean_per_min': (intensity_mean / auc_0_to_T) * 60 if (np.isfinite(auc_0_to_T) and auc_0_to_T > 0) else np.nan,
            'FUR_p90_per_min': (intensity_p90 / auc_0_to_T) * 60 if (np.isfinite(auc_0_to_T) and auc_0_to_T > 0) else np.nan,
        })

    return records


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description='PET Quantification from Geometry ROIs')
    parser.add_argument('--subject', type=str, required=True)
    parser.add_argument('--session', type=str, required=True)
    args = parser.parse_args()

    ensure_directories()

    log_dir = LOGNOTES_DIR
    log_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"quantify_geometry_rois_{timestamp}.log"

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )

    subject_id = args.subject
    session_id = args.session

    logger.info("=" * 60)
    logger.info("PET QUANTIFICATION FROM GEOMETRY ROIs")
    logger.info(f"Subject: {subject_id}, Session: {session_id}")
    logger.info("=" * 60)

    # --- Blinding key ---
    blinding_map = load_blinding_key()
    timepoint = get_timepoint(subject_id, session_id, blinding_map)
    logger.info(f"Timepoint: {timepoint}")

    partner_session = get_partner_session(subject_id, session_id, blinding_map)
    logger.info(f"Partner session: {partner_session}")

    # --- Load ROI data (in cropped CT space) ---
    roi_dir = TOTALSEG_ROI_DIR / f"{subject_id}_{session_id}"
    labeled_file = roi_dir / "tooth_shells_geometry.nii.gz"
    if not labeled_file.exists():
        logger.error(f"Geometry ROI not found: {labeled_file}")
        sys.exit(1)

    labeled_data, labeled_img = load_nifti(labeled_file)
    labeled_data = labeled_data.astype(np.int16)

    # Load exclusion info
    lookup_file = roi_dir / "tooth_shells_lookup.json"
    with open(lookup_file) as f:
        lookup = json.load(f)
    excluded_teeth = {int(k): v for k, v in lookup.get('excluded_teeth', {}).items()}
    logger.info(f"Excluded teeth (prosthetic): {list(excluded_teeth.keys())}")

    # --- Load PET (native space — all extraction happens here) ---
    from utils.io_utils import find_pet_file
    session_dir = RAWDATA_DIR / subject_id / session_id
    pet_file = find_pet_file(session_dir)
    if pet_file is None:
        logger.error(f"No PET file found")
        sys.exit(1)

    logger.info(f"Loading PET: {pet_file.name}")
    pet_data, pet_img = load_nifti(pet_file)
    logger.info(f"PET shape: {pet_data.shape}, range: [{pet_data.min():.1f}, {pet_data.max():.1f}]")

    # --- Load per-tooth continuous masks (PET space, from Step 5 co-reg) ---
    cont_dir = roi_dir / "continuous_masks_PETspace"
    if cont_dir.exists() and any(cont_dir.glob("tooth_*_continuous.nii.gz")):
        logger.info(f"Loading per-tooth continuous masks from {cont_dir.name}...")
        continuous_masks = {}
        for mask_file in sorted(cont_dir.glob("tooth_*_continuous.nii.gz")):
            fdi = int(mask_file.stem.split('_')[1])
            mask_data, _ = load_nifti(mask_file)
            continuous_masks[fdi] = mask_data.astype(np.float32)
        logger.info(f"Loaded {len(continuous_masks)} continuous masks (teeth: {sorted(continuous_masks.keys())})")
        use_continuous = True
    else:
        logger.warning(f"No continuous masks found — falling back to NN resampling in CT space")
        use_continuous = False

    # --- PET JSON for scan timing ---
    pet_json = load_pet_json(subject_id, timepoint)
    scan_start_s = pet_json.get('ScanStart', 1800)
    frame_duration_ms = pet_json.get('FrameDuration', [1800000])
    if isinstance(frame_duration_ms, list):
        frame_duration_ms = frame_duration_ms[0]
    scan_end_s = scan_start_s + frame_duration_ms / 1000
    tissue_time_s = (scan_start_s + scan_end_s) / 2  # mid-scan

    logger.info(f"Scan window: {scan_start_s:.0f}-{scan_end_s:.0f}s, "
                f"tissue time: {tissue_time_s:.0f}s")

    # Check PET units from JSON
    pet_units = pet_json.get('Units', 'unknown')
    logger.info(f"PET stated units: {pet_units}")
    if pet_units != 'Bq/mL':
        logger.warning(f"Expected Bq/mL, got {pet_units}. Proceeding assuming Bq/mL.")

    # --- eCRF for SUV parameters ---
    ecrf_df = load_ecrf_data()
    suv_params = get_suv_parameters(ecrf_df, subject_id, timepoint)

    # --- Input function for TPR/FUR ---
    if_data = load_input_function(subject_id, timepoint)
    plasma_denom = None
    if if_data is not None:
        plasma_denom = compute_plasma_denominators(
            if_data, scan_start_s, scan_end_s, tissue_time_s
        )

    # --- Extract metrics ---
    if use_continuous:
        # Work in PET native space with continuous (partial-volume) masks
        voxel_vol_ml = get_voxel_volume_ml(pet_img)
        records = extract_tooth_metrics_weighted(
            pet_data, continuous_masks, suv_params, plasma_denom, voxel_vol_ml
        )
        logger.info(f"Extracted metrics using PV-weighted continuous masks in PET native space")
    else:
        # Legacy fallback: resample PET to CT space
        logger.info("Resampling PET to ROI (cropped CT) space (legacy fallback)...")
        pet_in_roi_space = resample_pet_to_roi(pet_data, pet_img, labeled_img)
        voxel_vol_ml = get_voxel_volume_ml(labeled_img)
        # Build continuous_masks from labeled volume (binary, no PV weighting)
        unique_fdi = np.unique(labeled_data)
        unique_fdi = unique_fdi[unique_fdi > 0]
        fallback_masks = {int(fdi): (labeled_data == fdi).astype(np.float32) for fdi in unique_fdi}
        records = extract_tooth_metrics_weighted(
            pet_in_roi_space, fallback_masks, suv_params, plasma_denom, voxel_vol_ml
        )

    # Add session metadata
    for rec in records:
        rec['subject_id'] = subject_id
        rec['session_id'] = session_id
        rec['timepoint'] = timepoint

    # --- Save per-tooth metrics ---
    df = pd.DataFrame(records)

    # Reorder columns
    id_cols = ['subject_id', 'session_id', 'timepoint', 'fdi_tooth', 'jaw']
    roi_cols = ['n_voxels', 'n_voxels_excluded_zero', 'roi_volume_ml']
    intensity_cols = ['intensity_mean_Bq_mL', 'intensity_median_Bq_mL',
                      'intensity_p90_Bq_mL', 'intensity_max_Bq_mL', 'intensity_std_Bq_mL']
    suv_cols = ['SUV_mean', 'SUV_median', 'SUV_p90', 'SUV_max']
    tpr_cols = ['TPR_mean', 'TPR_p90']
    fur_cols = ['FUR_mean_per_min', 'FUR_p90_per_min']
    all_cols = id_cols + roi_cols + intensity_cols + suv_cols + tpr_cols + fur_cols
    df = df[[c for c in all_cols if c in df.columns]]

    # Save to DerivedData
    output_file = roi_dir / "pet_metrics_per_tooth.csv"
    df.to_csv(output_file, index=False)
    logger.info(f"Per-tooth metrics saved: {output_file}")

    # Also save to Outputs for easy access
    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
    outputs_file = OUTPUTS_DIR / "tooth_level_metrics.csv"
    if outputs_file.exists():
        existing = pd.read_csv(outputs_file)
        # Remove existing rows for this session
        mask = ~((existing['subject_id'] == subject_id) & (existing['session_id'] == session_id))
        existing = existing[mask]
        df_out = pd.concat([existing, df], ignore_index=True)
    else:
        df_out = df
    df_out.to_csv(outputs_file, index=False)
    logger.info(f"Output saved: {outputs_file}")

    # --- Print summary ---
    tooth_rows = df[df['fdi_tooth'].apply(lambda x: isinstance(x, (int, np.integer)))]
    jaw_rows = df[df['fdi_tooth'].apply(lambda x: isinstance(x, str))]

    logger.info(f"\n{'='*60}")
    logger.info("PET METRICS SUMMARY")
    logger.info(f"{'='*60}")

    if len(tooth_rows) > 0:
        logger.info(f"\nPer-tooth ({len(tooth_rows)} teeth):")
        logger.info(f"  Intensity (Bq/mL): {tooth_rows['intensity_mean_Bq_mL'].min():.0f} - "
                     f"{tooth_rows['intensity_mean_Bq_mL'].max():.0f}")
        if 'SUV_mean' in tooth_rows and tooth_rows['SUV_mean'].notna().any():
            logger.info(f"  SUV mean: {tooth_rows['SUV_mean'].min():.3f} - "
                         f"{tooth_rows['SUV_mean'].max():.3f}")
        if 'TPR_mean' in tooth_rows and tooth_rows['TPR_mean'].notna().any():
            logger.info(f"  TPR mean: {tooth_rows['TPR_mean'].min():.3f} - "
                         f"{tooth_rows['TPR_mean'].max():.3f}")
        if 'FUR_mean_per_min' in tooth_rows and tooth_rows['FUR_mean_per_min'].notna().any():
            logger.info(f"  FUR mean: {tooth_rows['FUR_mean_per_min'].min():.6f} - "
                         f"{tooth_rows['FUR_mean_per_min'].max():.6f} per min")

    for _, row in jaw_rows.iterrows():
        logger.info(f"\n{row['fdi_tooth']}:")
        logger.info(f"  Intensity: {row['intensity_mean_Bq_mL']:.0f} Bq/mL")
        if np.isfinite(row.get('SUV_mean', np.nan)):
            logger.info(f"  SUV: {row['SUV_mean']:.3f}")
        if np.isfinite(row.get('TPR_mean', np.nan)):
            logger.info(f"  TPR: {row['TPR_mean']:.3f}")
        if np.isfinite(row.get('FUR_mean_per_min', np.nan)):
            logger.info(f"  FUR: {row['FUR_mean_per_min']:.6f} per min")
        logger.info(f"  n_voxels: {row['n_voxels']}, volume: {row['roi_volume_ml']:.2f} mL")

    # Note: PET-space ROI files (tooth_shells_geometry_PETspace.nii.gz,
    # peridental_*_PETspace.nii.gz, continuous masks) are now created by
    # Step 5 (run_step5_coreg_and_resample) in 02_run_geometry_pipeline.py

    logger.info(f"\n{'='*60}")
    logger.info("DONE")
    logger.info(f"{'='*60}")


if __name__ == "__main__":
    main()
